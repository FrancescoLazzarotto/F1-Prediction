{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09f81d1-11a6-42b8-a00b-6174e5510a7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "Cache directory does not exist! Please check for typos or create it first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastf1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m cache_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAhmed Essam\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdevelopment\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mf1cache\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfastf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m grand_prix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBahrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m current_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2025\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\checc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastf1\\req.py:262\u001b[0m, in \u001b[0;36mCache.enable_cache\u001b[1;34m(cls, cache_dir, ignore_version, force_renew, use_requests_cache)\u001b[0m\n\u001b[0;32m    259\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(cache_dir)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cache_dir):\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotADirectoryError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache directory does not exist! Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck for typos or create it first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_CACHE_DIR \u001b[38;5;241m=\u001b[39m cache_dir\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_IGNORE_VERSION \u001b[38;5;241m=\u001b[39m ignore_version\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: Cache directory does not exist! Please check for typos or create it first."
     ]
    }
   ],
   "source": [
    "import fastf1\n",
    "cache_location = 'C:\\\\Users\\\\Ahmed Essam\\\\development\\\\AI\\\\F1\\\\f1cache'\n",
    "fastf1.Cache.enable_cache(cache_location)\n",
    "grand_prix = 'Bahrain'\n",
    "current_year = 2025\n",
    "previous_year = current_year - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31381940-28d3-4c36-8f05-6a3a83cd32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastF1 2024 GP race session\n",
    "session_2024 = fastf1.get_session(previous_year, grand_prix, \"R\")\n",
    "session_2024.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb7cdb-b943-4ef1-980f-ad1c9e06efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2024.results.info()\n",
    "session_2024.results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12500f0a-f305-4914-b08b-21695847d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quali_2025 = fastf1.get_session(current_year, grand_prix, \"Q\")\n",
    "quali_2025.load()\n",
    "quali_2024 = fastf1.get_session(previous_year, grand_prix, \"Q\")\n",
    "quali_2024.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca35458-eec5-44b0-85c5-873d08d25597",
   "metadata": {},
   "outputs": [],
   "source": [
    "quali_2025.results.info()\n",
    "quali_2025.results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7292fb-39e6-4736-96ce-1c1ce855adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2024.laps.info()\n",
    "session_2024.laps.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df024c7-2ac9-4e1c-8cf4-ad5856b442cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2024.results.info()\n",
    "session_2024.results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2024.weather_data.info()\n",
    "session_2024.weather_data.describe()\n",
    "# session_2024.weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332059b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2024.laps['LapTime'].describe()\n",
    "average_laptimes = session_2024.laps.groupby('Driver')['LapTime'].mean().reset_index()\n",
    "\n",
    "# Sort the drivers from fastest (lowest avg lap time) to slowest\n",
    "average_laptimes = average_laptimes.sort_values(by='LapTime', ascending=True)\n",
    "\n",
    "print(average_laptimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2de5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "quali24 = quali_2024.results[['DriverNumber', 'TeamName', 'Q1', 'Q2', 'Q3', 'Position', 'Abbreviation']].copy()\n",
    "session24 = session_2024.results[['DriverNumber', 'Time', 'Status']].copy()\n",
    "\n",
    "# Join X and y on 'DriverNumber'\n",
    "data = pd.merge(quali24, session24, on='DriverNumber', how='inner')\n",
    "\n",
    "max_laptime = average_laptimes['LapTime'].max()\n",
    "data.loc[data['Status'] == 'Lapped', 'Time'] += max_laptime\n",
    "print(data['Time'])\n",
    "# Drop rows where 'Time' is NaT and get the corresponding 'Abbreviation'\n",
    "missing_time_abbr = data[data['Time'].isna()]['Abbreviation']\n",
    "print(\"Abbreviations with missing Time:\", missing_time_abbr.tolist())\n",
    "data = data.dropna(subset=['Time'])\n",
    "X_train = data[['TeamName', 'Q1', 'Q2', 'Q3', 'Position', 'Abbreviation']].copy()\n",
    "# X_train = data[[ 'Q1', 'Q2', 'Q3', 'Position', 'Abbreviation']].copy()\n",
    "\n",
    "y_train = data[['Time']].copy()\n",
    "y_train.dropna(inplace=True)\n",
    "y_train['Time'] = y_train['Time'].dt.total_seconds()\n",
    "# Build X_predict with Abbreviation\n",
    "X_predict = quali_2025.results[['TeamName', 'Q1', 'Q2', 'Q3', 'Position', 'Abbreviation']].copy()\n",
    "# X_predict = quali_2025.results[['Q1', 'Q2', 'Q3', 'Position', 'Abbreviation']].copy()\n",
    "\n",
    "# Save the Abbreviation for later display and remove it from features\n",
    "driver_abbr = X_predict['Abbreviation'].copy()\n",
    "X_predict = X_predict.drop(columns=['Abbreviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a439ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# winner_time = y_train['Time'].iloc[0]\n",
    "# y_train.iloc[1:, y_train.columns.get_loc('Time')] += winner_time\n",
    "y_train.loc[0, 'Time'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train, y_train)\n",
    "print(\"the train std of Q2 and Q3 is: \", X_train[['Q2', 'Q3']].std())\n",
    "print(\"the predict std of Q2 and Q3 is: \", X_predict[['Q2', 'Q3']].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35be8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns=['TeamName'], prefix='Team')\n",
    "X_predict = pd.get_dummies(X_predict, columns=['TeamName'], prefix='Team')\n",
    "X_predict.rename(columns={'Team_Racing Bulls': 'Team_RB'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a66c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Q2'] = X_train['Q2'].fillna(X_train['Q1'] + pd.Timedelta(seconds=10))\n",
    "X_train['Q3'] = X_train['Q3'].fillna(X_train['Q2'] + pd.Timedelta(seconds=10))\n",
    "# X_train = X_train[~X_train['Abbreviation'].isin(['ALB', 'ZHO', 'RIC'])]\n",
    "X_train = X_train.drop(columns=['Abbreviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8082318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align y_train with the remaining rows in X_train\n",
    "y_train = y_train.loc[X_train.index]\n",
    "\n",
    "# Reset the index for both X_train and y_train to ensure they match\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict['Q2'] = X_predict['Q2'].fillna(X_predict['Q1'] + pd.Timedelta(seconds=10))\n",
    "X_predict['Q3'] = X_predict['Q3'].fillna(X_predict['Q2'] + pd.Timedelta(seconds=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d409ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Q1'] = X_train['Q1'].dt.total_seconds()\n",
    "X_train['Q2'] = X_train['Q2'].dt.total_seconds()\n",
    "X_train['Q3'] = X_train['Q3'].dt.total_seconds()\n",
    "X_predict['Q1'] = X_predict['Q1'].dt.total_seconds()\n",
    "X_predict['Q2'] = X_predict['Q2'].dt.total_seconds()\n",
    "X_predict['Q3'] = X_predict['Q3'].dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1667fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([X_train, y_train], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# # Feature engineering\n",
    "# X_train['Q1_Q2_diff'] = X_train['Q1'] - X_train['Q2']\n",
    "# X_train['Q2_Q3_diff'] = X_train['Q2'] - X_train['Q3']\n",
    "# X_predict['Q1_Q2_diff'] = X_predict['Q1'] - X_predict['Q2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "# rf = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_split=2, random_state=42)\n",
    "# gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=100,random_state=10)\n",
    "xgb_model = xgb.XGBRegressor(n_estimators = 100, random_state=10)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=10)\n",
    "# xgb_model = xgb.XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=42, verbosity=0)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingRegressor([\n",
    "    ('rf', rf),\n",
    "    ('gb', gb),\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "# Use LeaveOneOut cross-validation due to small dataset\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Calculate cross-validation scores for each model\n",
    "models = {'Random Forest': rf, 'Gradient Boosting': gb, 'XGBoost': xgb_model, 'Ensemble': ensemble}\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train['Time'], \n",
    "                            scoring='neg_mean_absolute_error',\n",
    "                            cv=loo)\n",
    "    mae = -scores.mean()\n",
    "    cv_results[name] = mae\n",
    "    print(f'{name} CV MAE: {mae:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:06.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize numerical features in X_train\n",
    "numerical_features = ['Q1', 'Q2', 'Q3', 'Position']\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "\n",
    "# Standardize numerical features in X_predict using the same scaler\n",
    "X_predict[numerical_features] = scaler.transform(X_predict[numerical_features])\n",
    "\n",
    "# Standardize the target variable\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "print('Data standardization completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bddf245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model training and prediction code to handle scaled data\n",
    "models = {'Random Forest': rf, 'Gradient Boosting': gb, 'XGBoost': xgb_model ,'Ensemble': ensemble}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit the model with scaled data\n",
    "    model.fit(X_train, y_train_scaled.ravel())\n",
    "    \n",
    "    # Make predictions and inverse transform to original scale\n",
    "    pred_scaled = model.predict(X_predict)\n",
    "    pred = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "    pred_adjusted =pred\n",
    "    \n",
    "    \n",
    "    predictions[name] = pred_adjusted\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Abbreviation': driver_abbr,\n",
    "        'Predicted_Race_Time_sec': pred_adjusted,\n",
    "        'Predicted_Race_Time': [seconds_to_time(s) for s in pred_adjusted]\n",
    "    })\n",
    "    \n",
    "    # Sort and display results\n",
    "    results_sorted = results.sort_values('Predicted_Race_Time_sec')\n",
    "    print(f\"\\n{name} Predictions:\")\n",
    "    print(results_sorted[['Abbreviation', 'Predicted_Race_Time']])\n",
    "    print(f\"MAE from cross-validation: {cv_results[name]:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea732681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for each model that supports it\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(f\"\\n{name} Feature Importances:\")\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        })\n",
    "        print(importances.sort_values('importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b849e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the winner (driver with lowest predicted race time)\n",
    "winner_abb = results_sorted['Abbreviation'].iloc[0]\n",
    "\n",
    "# Get the full driver name from qualifying data\n",
    "winner_data = quali_2025.results[quali_2025.results['Abbreviation'] == winner_abb]\n",
    "winner_name = winner_data['FullName'].iloc[0]\n",
    "\n",
    "print(f\"And the winner of the 2025 Bahrain Grand Prix is {winner_name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Get the winner (driver with lowest predicted race time)\n",
    "winner_abb = results_sorted['Abbreviation'].iloc[0]\n",
    "\n",
    "# Get the full driver name from qualifying data\n",
    "winner_data = quali_2025.results[quali_2025.results['Abbreviation'] == winner_abb]\n",
    "winner_name = winner_data['FullName'].iloc[0]\n",
    "\n",
    "markdown_text = f\"\"\"## 🏆 Race Winner Prediction\n",
    "\n",
    "\n",
    "### And the winner of the 2025 Bahrain Grand Prix is:\n",
    "# 🏎️ {winner_name}\"\"\"\n",
    "\n",
    "display(Markdown(markdown_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
